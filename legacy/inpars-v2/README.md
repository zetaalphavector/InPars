# Finetuning and evaluation of monoT5 on synthetic data generated by InPars using TPUs 

This README contains the instructions to reproduce results from [InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval](https://arxiv.org/abs/2301.01820)

## Getting the data

Let's first download all the data which are:

* Synthetic queries
* BM25 runs from
* Qrels
* Queries
* Corpus

```
pip install -U pyserini
pip install wget ir-measures
cd tpu/
nohup python -u download_data.py &
```

Be sure to have at least 350GB available on disk and follow this [guide](https://github.com/castorini/pygaggle/blob/master/docs/experiments-monot5-tpu.md#setup-environment-on-vm) to install T5 dependencies.

# Finetuning on synthetic data
Use a TPU v3-8 to train on each dataset:
```
nohup python -u train_inpars.py \
    --gcp_path path_to_gcp_bucket \
    --tpu_proj project_name \
    --tpu_name tpu_name &
```

# Retrieval
Use a TPU v3-8 to retrieve on each dataset:
```
nohup python run_t5_3B_inpars.py \
    --gcp_path path_to_gcp_bucket \
    --tpu_proj project_name \
    --tpu_name tpu_name &
```

# Download and evaluate

Download from the GPC bucket the retrieval's scores of each dataset and compute the IR metrics:
```
nohup python get_t5_3B_inpars.py \
    --gcp_path path_to_gcp_bucket \
    --tpu_proj project_name \
    --tpu_name tpu_name &
```

# Finetuned models
The models finetuned on BEIR are available on Huggingface:
https://huggingface.co/zeta-alpha-ai


# Citing this work
```
@misc{https://doi.org/10.48550/arxiv.2301.01820,
  doi = {10.48550/ARXIV.2301.01820},
  url = {https://arxiv.org/abs/2301.01820},
  author = {Jeronymo, Vitor and Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Zavrel, Jakub and Nogueira, Rodrigo},
  title = {InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval},
  publisher = {arXiv},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}
```
